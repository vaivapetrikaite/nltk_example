{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45da3f4f",
   "metadata": {},
   "source": [
    "# An example of a customer message parsing\n",
    "\n",
    "Here is the message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c8261",
   "metadata": {},
   "source": [
    "```Hi, my  name is Alice. I am calling from Alabama. I wonder if you could help me with my headaches. I have them frequently sometimes every day,  sometimes twice a week. I do not   know the reason. I have a very stressful job and have to communicate with many annoyed clients. Very tiring!!! My boss has yelled at me this morning. \n",
    "I cannot    concentrate because of the issues with my head.. My dad died three months ago because of kidney failure. I kept taking ibuprofen but it does not help me any more. Do you have alternatives that might help with my headache?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455442d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message=\"Hi, my  name is Alice. I am calling from Alabama. I wonder if you could help me with my headaches. I have them frequently sometimes every day,  sometimes twice a week. I do not   know the reason. I have a very stressful job and have to communicate with many annoyed clients. Very tiring!!! My boss has yelled at me this morning.  I cannot    concentrate because of the issues with my head.. My dad died three months ago because of kidney failure. I kept taking ibuprofen but it does not help me any more. Do you have alternatives that might help with my headache?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308cc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e8cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer=RegexpTokenizer(\"[\\.\\?\\!]\", gaps=True)\n",
    "sent_list=tokenizer.tokenize(message)\n",
    "sent_end_list=RegexpTokenizer(\"[^\\.\\?\\!]\", gaps=True).tokenize(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755f1ec",
   "metadata": {},
   "source": [
    "I borrowed  a piece of code from here https://stackoverflow.com/questions/28618400/how-to-identify-the-subject-of-a-sentence and adapted to my own needs. I use the functions to extract subjects and objects  from the sentence first. I use the subjects to identify the sentences that might be related to the condition of a customer (sentences with I, my body parts, and family memebers (in case of inherited disseases)). The sentences that have other subjects, can be ignored. To be more precise  with the subjects and objects, I extract nouns and adjectives from the seprated parts. Later, I need to check what words describe what I need (look at the definitions at wordnet). Keywords are selected according to the criteria of mine (keyword list that matches the definitions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b63b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(pos):\n",
    "    return pos[:2]=='NN'\n",
    "\n",
    "def is_adj(pos):\n",
    "    return pos[:2]=='JJ'\n",
    " #getting subjects of sentences\n",
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"subj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            sent_subj=doc[start:end].text           \n",
    "            if (sent_subj=='I'):\n",
    "                subj=sent_subj\n",
    "                return lemmatizer.lemmatize(subj)\n",
    "            elif (sent_subj==None):\n",
    "                subj==\"0\"\n",
    "                return subj\n",
    "            else:\n",
    "                tok_sent_subj=word_tokenize(sent_subj)\n",
    "                subj=[word for (word, pos) in pos_tag(tok_sent_subj) if is_noun(pos)]\n",
    "                if not subj:\n",
    "                    return None\n",
    "                else:     \n",
    "                    return lemmatizer.lemmatize(subj[0])\n",
    "#getting nouns and adjectives from the first object phrase in a sentence\n",
    "def get_object_phrase_nouns_adj(doc):\n",
    "    for token in doc:\n",
    "        if (\"dobj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            sent_subj=doc[start:end].text           \n",
    "            tok_sent_subj=word_tokenize(sent_subj)\n",
    "            subj=[word for (word, pos) in pos_tag(tok_sent_subj) if (is_noun(pos) |is_adj(pos)) ]\n",
    "            if not subj:\n",
    "                return None\n",
    "            else:\n",
    "                return list(map(lemmatizer.lemmatize, subj))\n",
    "\n",
    "#getting nouns and adjectives from the second object phrase in a sentence\n",
    "def get_second_object_phrase_nouns_adj(doc):\n",
    "    for token in doc:\n",
    "        if (\"pobj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            sent_subj=doc[start:end].text           \n",
    "            tok_sent_subj=word_tokenize(sent_subj)\n",
    "            subj=[word for (word, pos) in pos_tag(tok_sent_subj) if (is_noun(pos) |is_adj(pos))]\n",
    "            if not subj:\n",
    "                return None\n",
    "            else:\n",
    "                return list(map(lemmatizer.lemmatize, subj))\n",
    "\n",
    "            \n",
    "family_members=['father', 'dad', 'mom', 'sis', 'bro', 'granny', 'mother', 'son', 'daughter', 'aunt', 'uncle', 'cousin', 'grandfather', 'grandmother', \\\n",
    "                'granddaughter', 'grandson']\n",
    "#finding family members in the lsit of words\n",
    "def word_def_family(doc):\n",
    "    if doc != None:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(doc):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "        inters=list(set(synonyms) & set(family_members))\n",
    "        return str(len(inters))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#finding specific keywords in the list of words     \n",
    "def word_def_keywords(doc):\n",
    "    if doc != None:\n",
    "        extract_w=[]\n",
    "        for word in doc:\n",
    "            define=wordnet.synsets(word)[0].definition()\n",
    "            list_w=word_tokenize(define)\n",
    "            fin_list=list(map(lemmatizer.lemmatize, list_w))\n",
    "            inters=list(set(fin_list) & set(['body', 'organ','stress', 'nerve', 'anxiety', 'medicine', 'drug']))\n",
    "            detect=len(inters)\n",
    "            if detect>0:\n",
    "                extract_w.append(word)\n",
    "        return extract_w\n",
    "    else: \n",
    "        return None\n",
    "#to deal with the list of lists. \n",
    "def flatten(doc):\n",
    "    if doc != None:\n",
    "        flatter_list = list()\n",
    "        if len(doc)>1:\n",
    "            for sub_list in doc:\n",
    "                if sub_list != None:\n",
    "                    flatter_list += sub_list\n",
    "        else:\n",
    "            flatter_list += doc[0]\n",
    "        return flatter_list\n",
    "    else: \n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce42fa",
   "metadata": {},
   "source": [
    "This piece of code extracts all the nouns and adjectives from the sentence subjects and objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a473df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_list=list()\n",
    "obj_noun_adv=list()\n",
    "\n",
    "for sentence in sent_list:\n",
    "    doc = nlp(sentence)\n",
    "    a = get_subject_phrase(doc)\n",
    "    b = get_object_phrase_nouns_adj(doc)\n",
    "    c = get_second_object_phrase_nouns_adj(doc)\n",
    "    obj_noun_adv.append([b,c])\n",
    "    subj_list.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe24db",
   "metadata": {},
   "source": [
    "I put everything into the Pandas dataframe because it is easier for me to follow what sentences to delete: one line coresponds to one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cf3925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>nouns_adj</th>\n",
       "      <th>family</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>[Alabama]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>[headache]</td>\n",
       "      <td>0</td>\n",
       "      <td>[headache]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>[reason]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>[stressful, job, many, client]</td>\n",
       "      <td>0</td>\n",
       "      <td>[stressful]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subj                       nouns_adj family     keywords\n",
       "1    I                       [Alabama]      0           []\n",
       "2    I                      [headache]      0   [headache]\n",
       "3    I                              []      0           []\n",
       "4    I                        [reason]      0           []\n",
       "5    I  [stressful, job, many, client]      0  [stressful]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({\"subj\":subj_list,\"nouns_adj\":obj_noun_adv})\n",
    "df['nouns_adj']=df['nouns_adj'].apply(flatten)\n",
    "df['family']=df['subj'].apply(word_def_family)\n",
    "df=df[((df['family'] != '0') | (df['subj']=='I')) & (~df['subj'].isnull())]\n",
    "df['keywords']=df['nouns_adj'].apply(word_def_keywords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ab96e",
   "metadata": {},
   "source": [
    "This is **the list of keywords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fab7680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['headache', 'stressful', 'head', 'kidney', 'ibuprofen']\n"
     ]
    }
   ],
   "source": [
    "print(flatten(df['keywords'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822d77c",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "I selected the list of keywords that might present a pharmasist with some questions, e.g. 'Is ibuprofen safe for people with kidney problems?' However, there are a several things to be noticed\n",
    "1. Currently, the term *headache* and the term *kidney* have the same weight. Actually the patient called because of her headaches and kidney problems are potential (because of family traits). This has to be taken into account. \n",
    "2. I did not perform the analysis of verbal phrases. it is important to know that *ibuprofen did not help*. The pharmacists would be encouraged to search for alternatives, to ask what alternatives of ibuprofen were used, what concentration of ibuprofen was taken, etc. \n",
    "3. The gender, time of request (e.g. full night moon, Friday night, etc. ), place (maybe there was some unusual pollution in Alabama), device from which it was contacted (phone, computer) could be processed. \n",
    "4. I did not make any use of explamation marks and broken sentences like *Very tiring*. They show lack of concentration, stress and hurry/tiredness. \n",
    "5. **All in all**, this lady needs a recommendation to go to a doctor and ask for drugs like Xanax. It is a question if a pharmacist wants to give such a recommendation. More thorough analysis has to be performed to propose this suggestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cd06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
